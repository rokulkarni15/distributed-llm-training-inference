{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Distributed LLM Training Experiments\n",
    "\n",
    "This notebook runs an exhaustive set of training jobs to compare different DeepSpeed ZeRO stages across multiple GPU configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPARING GLAIVE-CODE-ASSISTANT DATASET\n",
      "======================================================================\n",
      "\n",
      "[1/4] Loading dataset from HuggingFace...\n",
      "Dataset: glaiveai/glaive-code-assistant\n",
      "✓ Loaded 136,109 samples\n",
      "\n",
      "[2/4] Using full dataset (136,109 samples)\n",
      "\n",
      "[3/4] Formatting conversations for Llama 2...\n",
      "Converting question-answer pairs to Llama 2 chat format...\n",
      "Formatting conversations: 100%|█| 136109/136109 [00:05<00:00, 23943.43 examples/\n",
      "✓ Formatted 136,109 conversations\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXAMPLE FORMATTED CONVERSATION (first 500 chars):\n",
      "----------------------------------------------------------------------\n",
      "<s>[INST] How can I output bold text in Bash? I have a Bash script that prints some text to the screen using the `echo \"Some Text\"` command. Is there a way I can format the text to make it bold? [/INST] Yes, you can format the output text in Bash to make it bold. Bash allows you to use special escape sequences for text decoration. To make some text bold in bash, you would use the escape sequence `\\033[1m`, and to reset the formatting, you would use `\\033[0m`. \n",
      "\n",
      "Here's how you can update your `ec...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[4/4] Saving preprocessed dataset...\n",
      "Output path: ./data/glaive_code_full\n",
      "Saving the dataset (1/1 shards): 100%|█| 136109/136109 [00:00<00:00, 207814.99 e\n",
      "✓ Saved successfully\n",
      "✓ Dataset size: 452.2 MB\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Samples:        136,109\n",
      "Size:           452.2 MB\n",
      "Location:       ./data/glaive_code_full\n",
      "Format:         Llama 2 chat format\n",
      "======================================================================\n",
      "\n",
      "✓ Dataset preparation complete!\n",
      "\n",
      "To use in training, load from: ./data/glaive_code_full\n",
      "Example:\n",
      "    from datasets import load_from_disk\n",
      "    dataset = load_from_disk('./data/glaive_code_full')\n",
      "\n",
      "✓ SUCCESS! Dataset is ready for training.\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/prepare_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Training (1 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python training/train_baseline.py \\n\n",
    "    --dataset_path ./data/glaive_code_full \\n\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSpeed ZeRO Stage 1 Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-1 with 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\n",
    "deepspeed --num_gpus=1 training/train_deepspeed_zero1.py --dataset_path ./data/glaive_code_full --deepspeed_config configs/ds_config_zero1.json --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-1 with 2 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\n",
    "deepspeed --num_gpus=2 training/train_deepspeed_zero1.py --dataset_path ./data/glaive_code_full --deepspeed_config configs/ds_config_zero1.json --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-1 with 3 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\n",
    "deepspeed --num_gpus=3 training/train_deepspeed_zero1.py --dataset_path ./data/glaive_code_full --deepspeed_config configs/ds_config_zero1.json --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-1 with 4 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\n",
    "deepspeed --num_gpus=4 training/train_deepspeed_zero1.py --dataset_path ./data/glaive_code_full --deepspeed_config configs/ds_config_zero1.json --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSpeed ZeRO Stage 2 Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-2 with 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-13 23:34:48,147] [WARNING] [runner.py:232:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "Detected VISIBLE_DEVICES=0 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.\n",
      "[2025-11-13 23:34:48,147] [INFO] [runner.py:630:main] cmd = /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None --log_level=info train_deepspeed_zero2.py --dataset_path data/glaive_code_full --deepspeed_config ../configs/ds_config_zero2.json --num_train_epochs 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-13 23:35:01,327] [INFO] [launch.py:162:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2025-11-13 23:35:01,327] [INFO] [launch.py:168:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2025-11-13 23:35:01,327] [INFO] [launch.py:179:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2025-11-13 23:35:01,327] [INFO] [launch.py:180:main] dist_world_size=1\n",
      "[2025-11-13 23:35:01,327] [INFO] [launch.py:184:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "[2025-11-13 23:35:01,344] [INFO] [launch.py:272:main] process 2241266 spawned with command: ['/home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/python3', '-u', 'train_deepspeed_zero2.py', '--local_rank=0', '--dataset_path', 'data/glaive_code_full', '--deepspeed_config', '../configs/ds_config_zero2.json', '--num_train_epochs', '1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DEEPSPEED ZeRO-2 TRAINING\n",
      "======================================================================\n",
      "\n",
      "Experiment: zero2_1gpu\n",
      "GPUs: 1\n",
      "ZeRO Stage: 2\n",
      "Config: ../configs/ds_config_zero2.json\n",
      "Output: ./checkpoints/zero2_1gpu\n",
      "\n",
      "NOTE: Using DeepSpeed ZeRO-2 for optimizer + gradient state partitioning\n",
      "Expected: Lower memory usage than ZeRO-1, similar speed on 1 GPU\n",
      "\n",
      "Single GPU Training\n",
      "  GPU: Tesla V100-SXM2-32GB\n",
      "  Memory: 34.1 GB\n",
      "\n",
      "[1/5] Loading tokenizer...\n",
      "Tokenizer loaded\n",
      "\n",
      "[2/5] Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "\n",
      "[3/5] Applying LoRA (r=16)...\n",
      "trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.2484\n",
      "LoRA applied\n",
      "\n",
      "[4/5] Loading dataset...\n",
      "Dataset ready: 136,109 samples\n",
      "\n",
      "[5/5] Configuring training with DeepSpeed...\n",
      "✓ Trainer configured with DeepSpeed ZeRO-2\n",
      "\n",
      "Effective batch size: 1\n",
      "  = 1 (per_device) × 1 (grad_accum) × 1 (GPUs)\n",
      "\n",
      "======================================================================\n",
      "STARTING TRAINING: ZERO2_1GPU\n",
      "======================================================================\n",
      "\n",
      "[2025-11-13 23:35:48,104] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/136109 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "  0%|          | 3/136109 [00:02<26:39:45,  1.42it/s]/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9429, 'grad_norm': 0.5983909368515015, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8647, 'grad_norm': 1.2069456577301025, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7048, 'grad_norm': 0.969007670879364, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.71, 'grad_norm': 0.6146083474159241, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7665, 'grad_norm': 0.5691312551498413, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7654, 'grad_norm': 0.4435938000679016, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.9338, 'grad_norm': 1.056951880455017, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7799, 'grad_norm': 0.6003472805023193, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.9445, 'grad_norm': 0.4042268991470337, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6808, 'grad_norm': 0.7908876538276672, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6573, 'grad_norm': 0.7943319082260132, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8843, 'grad_norm': 0.5668656229972839, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.748, 'grad_norm': 0.5840670466423035, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7557, 'grad_norm': 0.5759570598602295, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.709, 'grad_norm': 0.5735697150230408, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7803, 'grad_norm': 0.6990373730659485, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7216, 'grad_norm': 0.3643207252025604, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.9469, 'grad_norm': 0.6413542032241821, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7267, 'grad_norm': 0.42799803614616394, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.813, 'grad_norm': 0.6777283549308777, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.714, 'grad_norm': 0.5489429235458374, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7845, 'grad_norm': 0.6366086602210999, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7238, 'grad_norm': 0.5261906385421753, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6882, 'grad_norm': 0.5342462062835693, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8236, 'grad_norm': 0.4934287965297699, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7276, 'grad_norm': 0.9153556823730469, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6814, 'grad_norm': 0.6567797064781189, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7016, 'grad_norm': 0.5620816946029663, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7125, 'grad_norm': 0.8300419449806213, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7209, 'grad_norm': 0.5526116490364075, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7383, 'grad_norm': 0.6379275918006897, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7652, 'grad_norm': 0.7383254170417786, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.9746, 'grad_norm': 0.68204265832901, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7105, 'grad_norm': 0.5811329483985901, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6285, 'grad_norm': 0.7136653661727905, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7991, 'grad_norm': 0.5820700526237488, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8633, 'grad_norm': 0.5503109693527222, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.5804, 'grad_norm': 0.5153825879096985, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7331, 'grad_norm': 0.5380816459655762, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8153, 'grad_norm': 0.5726666450500488, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7292, 'grad_norm': 0.6595020890235901, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8559, 'grad_norm': 0.7035453915596008, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8604, 'grad_norm': 0.5993450880050659, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8938, 'grad_norm': 0.6085866093635559, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.584, 'grad_norm': 0.5516183376312256, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6876, 'grad_norm': 0.6969989538192749, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7427, 'grad_norm': 0.6106416583061218, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8199, 'grad_norm': 0.6082800030708313, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6695, 'grad_norm': 0.860609233379364, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7276, 'grad_norm': 0.6468712091445923, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6458, 'grad_norm': 0.5741705298423767, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7373, 'grad_norm': 0.8438230156898499, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7603, 'grad_norm': 0.862142026424408, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6628, 'grad_norm': 0.3446486294269562, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8057, 'grad_norm': 0.7357738018035889, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7201, 'grad_norm': 0.8662071228027344, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8545, 'grad_norm': 0.5965161919593811, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7156, 'grad_norm': 0.5379759073257446, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6078, 'grad_norm': 0.3342971205711365, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.74, 'grad_norm': 0.42922475934028625, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7429, 'grad_norm': 0.7388801574707031, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6919, 'grad_norm': 0.7119267582893372, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7334, 'grad_norm': 0.5074682235717773, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6498, 'grad_norm': 0.5586152076721191, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.6787, 'grad_norm': 0.7403524518013, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8614, 'grad_norm': 0.5524610280990601, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7114, 'grad_norm': 0.7451117038726807, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7191, 'grad_norm': 0.5864301323890686, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.7985, 'grad_norm': 0.7163569927215576, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7696, 'grad_norm': 0.49130764603614807, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6537, 'grad_norm': 0.686177670955658, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7891, 'grad_norm': 0.694534182548523, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.738, 'grad_norm': 0.5229307413101196, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7279, 'grad_norm': 0.6571704745292664, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6845, 'grad_norm': 1.1583161354064941, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8277, 'grad_norm': 0.8382912874221802, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7568, 'grad_norm': 0.7205826044082642, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7078, 'grad_norm': 0.6556475758552551, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7583, 'grad_norm': 0.5424992442131042, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8069, 'grad_norm': 0.5694998502731323, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6912, 'grad_norm': 0.6083325743675232, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8063, 'grad_norm': 0.6306637525558472, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8108, 'grad_norm': 0.6755721569061279, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7317, 'grad_norm': 0.5252822637557983, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.777, 'grad_norm': 0.5743282437324524, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6921, 'grad_norm': 0.722793698310852, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7971, 'grad_norm': 0.6648502945899963, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.805, 'grad_norm': 0.8429359197616577, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6892, 'grad_norm': 0.8735092878341675, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6753, 'grad_norm': 0.5117878317832947, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8522, 'grad_norm': 0.733689546585083, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6585, 'grad_norm': 1.505370020866394, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6399, 'grad_norm': 0.3992004990577698, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7312, 'grad_norm': 0.8350051641464233, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7189, 'grad_norm': 0.5158593058586121, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.0381, 'grad_norm': 0.6031849980354309, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8183, 'grad_norm': 0.6935465931892395, 'learning_rate': 0.0002, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 980/136109 [07:39<12:02:00,  3.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.825, 'grad_norm': 0.7595116496086121, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7383, 'grad_norm': 0.8015678524971008, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6939, 'grad_norm': 0.7927616238594055, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7404, 'grad_norm': 0.5571944117546082, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8011, 'grad_norm': 0.6059478521347046, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7123, 'grad_norm': 0.6414658427238464, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7796, 'grad_norm': 0.6385908126831055, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8706, 'grad_norm': 1.821088194847107, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6274, 'grad_norm': 0.9049996137619019, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7857, 'grad_norm': 0.7886815071105957, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7431, 'grad_norm': 0.7908366322517395, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6842, 'grad_norm': 0.5990347266197205, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6687, 'grad_norm': 0.6677344441413879, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7465, 'grad_norm': 0.8754263520240784, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6265, 'grad_norm': 0.6268316507339478, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7897, 'grad_norm': 0.8427121639251709, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7384, 'grad_norm': 0.7105408906936646, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6211, 'grad_norm': 0.4876326620578766, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7695, 'grad_norm': 0.5687933564186096, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6291, 'grad_norm': 0.592475175857544, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.5648, 'grad_norm': 0.4889158010482788, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7757, 'grad_norm': 0.6115009188652039, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7305, 'grad_norm': 0.6773565411567688, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8082, 'grad_norm': 0.6970318555831909, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6972, 'grad_norm': 0.8273069262504578, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8001, 'grad_norm': 0.8604162931442261, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7313, 'grad_norm': 0.7839171886444092, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8701, 'grad_norm': 0.6247178316116333, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8134, 'grad_norm': 0.473337858915329, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7965, 'grad_norm': 0.8343665599822998, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6455, 'grad_norm': 0.5567319989204407, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6916, 'grad_norm': 0.752227246761322, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.702, 'grad_norm': 0.601226806640625, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7305, 'grad_norm': 0.6463136672973633, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6923, 'grad_norm': 0.6170961856842041, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7647, 'grad_norm': 0.7931126356124878, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.668, 'grad_norm': 0.7565171122550964, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6988, 'grad_norm': 0.6539785265922546, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6585, 'grad_norm': 0.6767796874046326, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7255, 'grad_norm': 0.6616297364234924, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8035, 'grad_norm': 0.6685508489608765, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.712, 'grad_norm': 0.9015835523605347, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6157, 'grad_norm': 0.7027674913406372, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6499, 'grad_norm': 0.7149755358695984, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7396, 'grad_norm': 0.584466278553009, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8116, 'grad_norm': 0.6545344591140747, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.9124, 'grad_norm': 0.5755041837692261, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7308, 'grad_norm': 0.72480309009552, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8579, 'grad_norm': 0.669347882270813, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7088, 'grad_norm': 0.6724730134010315, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8002, 'grad_norm': 0.7045941352844238, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8407, 'grad_norm': 0.5752504467964172, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.769, 'grad_norm': 0.6863864660263062, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7413, 'grad_norm': 0.47252988815307617, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.784, 'grad_norm': 0.7501031160354614, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6522, 'grad_norm': 0.4802650213241577, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6911, 'grad_norm': 0.9609680771827698, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6896, 'grad_norm': 0.7736042141914368, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7167, 'grad_norm': 0.6242709159851074, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6662, 'grad_norm': 0.6616993546485901, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8305, 'grad_norm': 0.6018266677856445, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8282, 'grad_norm': 0.6425794959068298, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.9076, 'grad_norm': 0.6676955819129944, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7065, 'grad_norm': 0.49863454699516296, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6955, 'grad_norm': 1.0236046314239502, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7206, 'grad_norm': 0.92563796043396, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7896, 'grad_norm': 0.7901719808578491, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.5723, 'grad_norm': 0.5487880110740662, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8241, 'grad_norm': 0.7135254144668579, 'learning_rate': 0.0002, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1930/136109 [15:15<13:34:27,  2.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7674, 'grad_norm': 0.7560083270072937, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8152, 'grad_norm': 0.7469887137413025, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7372, 'grad_norm': 0.9750287532806396, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6844, 'grad_norm': 0.6634159088134766, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7256, 'grad_norm': 0.6087028384208679, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7093, 'grad_norm': 0.596566379070282, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.594, 'grad_norm': 0.6430320143699646, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7352, 'grad_norm': 0.5753172636032104, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8181, 'grad_norm': 0.5369955897331238, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7185, 'grad_norm': 0.7640063762664795, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6714, 'grad_norm': 0.7889512777328491, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.6901, 'grad_norm': 0.8016597032546997, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.7315, 'grad_norm': 0.5822119116783142, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6822, 'grad_norm': 0.5915054082870483, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7655, 'grad_norm': 0.8382899165153503, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7404, 'grad_norm': 0.8932458162307739, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7721, 'grad_norm': 0.9300423860549927, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7679, 'grad_norm': 0.8147251605987549, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8057, 'grad_norm': 0.5130459666252136, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6106, 'grad_norm': 1.244625210762024, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7641, 'grad_norm': 0.691661536693573, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8667, 'grad_norm': 0.7766444683074951, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7357, 'grad_norm': 0.7964797019958496, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7982, 'grad_norm': 0.6232472658157349, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6786, 'grad_norm': 0.7778802514076233, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.767, 'grad_norm': 0.6247749924659729, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6638, 'grad_norm': 1.2627849578857422, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6675, 'grad_norm': 0.8971337080001831, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7605, 'grad_norm': 0.6306425333023071, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6353, 'grad_norm': 0.7883496880531311, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.5984, 'grad_norm': 0.5807790160179138, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7168, 'grad_norm': 0.7966337203979492, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8, 'grad_norm': 0.6640157103538513, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7199, 'grad_norm': 0.6222456097602844, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.5606, 'grad_norm': 1.0800970792770386, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7935, 'grad_norm': 0.4963873624801636, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7001, 'grad_norm': 0.5921919941902161, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6781, 'grad_norm': 0.8639154434204102, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8256, 'grad_norm': 0.6906176209449768, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7313, 'grad_norm': 1.4535130262374878, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.736, 'grad_norm': 0.9266960620880127, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8137, 'grad_norm': 0.5792041420936584, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6391, 'grad_norm': 1.3267581462860107, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7364, 'grad_norm': 0.7063833475112915, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7801, 'grad_norm': 0.6700479388237, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8617, 'grad_norm': 0.8098267912864685, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8515, 'grad_norm': 0.4737442135810852, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7505, 'grad_norm': 0.7244400978088379, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8923, 'grad_norm': 0.8996107578277588, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7337, 'grad_norm': 0.7131708264350891, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7404, 'grad_norm': 0.7673705220222473, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7522, 'grad_norm': 1.2575526237487793, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6638, 'grad_norm': 0.9365674257278442, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6517, 'grad_norm': 0.7780330181121826, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7781, 'grad_norm': 0.7804220914840698, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7765, 'grad_norm': 0.9597077369689941, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7119, 'grad_norm': 0.8279045224189758, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7073, 'grad_norm': 0.9831622242927551, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6833, 'grad_norm': 0.49834713339805603, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7332, 'grad_norm': 0.8117774724960327, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6943, 'grad_norm': 0.5908442139625549, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7772, 'grad_norm': 0.6800764203071594, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6059, 'grad_norm': 1.196170687675476, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.5865, 'grad_norm': 0.9548884630203247, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7964, 'grad_norm': 0.90799480676651, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6448, 'grad_norm': 0.5748696327209473, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.698, 'grad_norm': 0.7537549138069153, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.604, 'grad_norm': 0.8098087906837463, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6734, 'grad_norm': 0.6647443771362305, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6469, 'grad_norm': 0.6071893572807312, 'learning_rate': 0.0002, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2856/136109 [22:31<12:31:38,  2.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8802, 'grad_norm': 0.7793275117874146, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6845, 'grad_norm': 1.0517269372940063, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8086, 'grad_norm': 0.8578470945358276, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6924, 'grad_norm': 0.6484431624412537, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7156, 'grad_norm': 1.0211008787155151, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7601, 'grad_norm': 0.7176656723022461, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.5683, 'grad_norm': 0.8353300094604492, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7841, 'grad_norm': 1.085649847984314, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7207, 'grad_norm': 0.7614701986312866, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7413, 'grad_norm': 0.6306690573692322, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6833, 'grad_norm': 0.6472669839859009, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7762, 'grad_norm': 0.6198179125785828, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.5969, 'grad_norm': 0.6498574018478394, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6061, 'grad_norm': 0.8106541633605957, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6749, 'grad_norm': 0.8801522850990295, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.723, 'grad_norm': 0.5911030173301697, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7777, 'grad_norm': 0.6678886413574219, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6156, 'grad_norm': 0.6812381744384766, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8825, 'grad_norm': 0.6762073040008545, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6489, 'grad_norm': 0.5708314180374146, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6753, 'grad_norm': 0.8758596181869507, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8241, 'grad_norm': 1.8431464433670044, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8595, 'grad_norm': 0.673043429851532, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7345, 'grad_norm': 0.9349489212036133, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.9866, 'grad_norm': 0.7051912546157837, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8595, 'grad_norm': 0.7777061462402344, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.8043, 'grad_norm': 0.8364993333816528, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.7907, 'grad_norm': 0.9255082011222839, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.5537, 'grad_norm': 0.7006760835647583, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.5596, 'grad_norm': 0.6851425766944885, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.6984, 'grad_norm': 1.0019471645355225, 'learning_rate': 0.0002, 'epoch': 0.02}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\n",
    "deepspeed --num_gpus=1 train_deepspeed_zero2.py --dataset_path data/glaive_code_full --deepspeed_config ../configs/ds_config_zero2.json --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-2 with 2 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 23:28:47,533] [WARNING] [runner.py:232:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "Detected VISIBLE_DEVICES=0 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.\n",
      "[2025-11-12 23:28:47,534] [INFO] [runner.py:630:main] cmd = /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None --log_level=info train_deepspeed_zero2.py --dataset_path data/glaive_code_full --deepspeed_config ../configs/ds_config_zero2.json --num_train_epochs 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 23:28:57,665] [INFO] [launch.py:162:main] WORLD INFO DICT: {'localhost': [0, 1]}\n",
      "[2025-11-12 23:28:57,666] [INFO] [launch.py:168:main] nnodes=1, num_local_procs=2, node_rank=0\n",
      "[2025-11-12 23:28:57,666] [INFO] [launch.py:179:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n",
      "[2025-11-12 23:28:57,666] [INFO] [launch.py:180:main] dist_world_size=2\n",
      "[2025-11-12 23:28:57,666] [INFO] [launch.py:184:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "[2025-11-12 23:28:57,695] [INFO] [launch.py:272:main] process 4091255 spawned with command: ['/home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/python3', '-u', 'train_deepspeed_zero2.py', '--local_rank=0', '--dataset_path', 'data/glaive_code_full', '--deepspeed_config', '../configs/ds_config_zero2.json', '--num_train_epochs', '1']\n",
      "[2025-11-12 23:28:57,719] [INFO] [launch.py:272:main] process 4091256 spawned with command: ['/home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/python3', '-u', 'train_deepspeed_zero2.py', '--local_rank=1', '--dataset_path', 'data/glaive_code_full', '--deepspeed_config', '../configs/ds_config_zero2.json', '--num_train_epochs', '1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DEEPSPEED ZeRO-2 TRAINING\n",
      "======================================================================\n",
      "\n",
      "Experiment: zero2_2gpu\n",
      "GPUs: 2\n",
      "ZeRO Stage: 2\n",
      "Config: ../configs/ds_config_zero2.json\n",
      "Output: ./checkpoints/zero2_2gpu\n",
      "\n",
      "NOTE: Using DeepSpeed ZeRO-2 for optimizer + gradient state partitioning\n",
      "Expected: Greater memory savings + 2x speedup with 2 GPUs\n",
      "\n",
      "Distributed Training: 2 GPUs\n",
      "  GPU 0: Tesla V100-SXM2-32GB\n",
      "  GPU 1: Tesla V100-SXM2-32GB\n",
      "\n",
      "[1/5] Loading tokenizer...\n",
      "Tokenizer loaded\n",
      "\n",
      "[2/5] Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 33.01it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 33.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "\n",
      "[3/5] Applying LoRA (r=16)...\n",
      "trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.2484\n",
      "LoRA applied\n",
      "\n",
      "[4/5] Loading dataset...\n",
      "Dataset ready: 136,109 samples\n",
      "\n",
      "[5/5] Configuring training with DeepSpeed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/training/train_deepspeed_zero2.py\", line 393, in <module>\n",
      "    main()\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/training/train_deepspeed_zero2.py\", line 261, in main\n",
      "    training_args = TrainingArguments(\n",
      "  File \"<string>\", line 135, in __init__\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/transformers/training_args.py\", line 1811, in __post_init__\n",
      "    self.device\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/transformers/training_args.py\", line 2355, in device\n",
      "    return self._setup_devices\n",
      "  File \"/usr/lib64/python3.9/functools.py\", line 993, in __get__\n",
      "    val = self.func(instance)\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/transformers/training_args.py\", line 2282, in _setup_devices\n",
      "    self.distributed_state = PartialState(**accelerator_state_kwargs)\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/accelerate/state.py\", line 216, in __init__\n",
      "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
      "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
      "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
      "    torch.distributed.init_process_group(backend, **kwargs)\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
      "    func_return = func(*args, **kwargs)\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 1717, in init_process_group\n",
      "    default_pg, _ = _new_process_group_helper(\n",
      "  File \"/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 2074, in _new_process_group_helper\n",
      "    eager_backend.eager_connect_single_device(device_id)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/home/chaudhari.paw/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "[W1112 23:29:31.821613367 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-12 23:29:33,756] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 4091255\n",
      "[2025-11-12 23:29:33,959] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 4091256\n",
      "[2025-11-12 23:29:33,959] [ERROR] [launch.py:341:sigkill_handler] ['/home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/python3', '-u', 'train_deepspeed_zero2.py', '--local_rank=1', '--dataset_path', 'data/glaive_code_full', '--deepspeed_config', '../configs/ds_config_zero2.json', '--num_train_epochs', '1'] exits with return code = 1\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\\ndeepspeed --num_gpus=2 train_deepspeed_zero2.py --dataset_path data/glaive_code_full --deepspeed_config ../configs/ds_config_zero2.json --num_train_epochs 1\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdeepspeed --num_gpus=2 train_deepspeed_zero2.py --dataset_path data/glaive_code_full --deepspeed_config ../configs/ds_config_zero2.json --num_train_epochs 1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/distributed-llm-training-inference/myenv/lib64/python3.9/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\\ndeepspeed --num_gpus=2 train_deepspeed_zero2.py --dataset_path data/glaive_code_full --deepspeed_config ../configs/ds_config_zero2.json --num_train_epochs 1\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\n",
    "deepspeed --num_gpus=2 train_deepspeed_zero2.py --dataset_path data/glaive_code_full --deepspeed_config ../configs/ds_config_zero2.json --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-2 with 3 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\n",
    "deepspeed --num_gpus=3 training/train_deepspeed_zero2.py --dataset_path ./data/glaive_code_full --deepspeed_config configs/ds_config_zero2.json --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-2 with 4 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\n",
    "deepspeed --num_gpus=4 training/train_deepspeed_zero2.py --dataset_path ./data/glaive_code_full --deepspeed_config configs/ds_config_zero2.json --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSpeed ZeRO Stage 3 Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-3 with 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\n",
    "deepspeed --num_gpus=1 training/train_deepspeed_zero3.py --dataset_path ./data/glaive_code_full --deepspeed configs/ds_config_zero3.json --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-3 with 2 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\n",
    "deepspeed --num_gpus=2 training/train_deepspeed_zero3.py --dataset_path ./data/glaive_code_full --deepspeed configs/ds_config_zero3.json --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-3 with 3 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\n",
    "deepspeed --num_gpus=3 training/train_deepspeed_zero3.py --dataset_path ./data/glaive_code_full --deepspeed configs/ds_config_zero3.json --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-3 with 4 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /home/chaudhari.paw/distributed-llm-training-inference/myenv/bin/activate\n",
    "deepspeed --num_gpus=4 training/train_deepspeed_zero3.py --dataset_path ./data/glaive_code_full --deepspeed configs/ds_config_zero3.json --num_train_epochs 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_kernel)",
   "language": "python",
   "name": "my_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
